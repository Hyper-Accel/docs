{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-hyperaccel-documentation","title":"Welcome to HyperAccel Documentation","text":""},{"location":"#about-hyperaccel","title":"About HyperAccel","text":"<p>HyperAccel is a pioneering semiconductor company specializing in the design and development of LLM-optimized chips. We focus on creating next-generation hardware solutions specifically engineered to accelerate large language model inference workloads.</p> <p>Our mission is to revolutionize AI computing by delivering hardware that fundamentally understands the unique computational patterns and requirements of large language models, enabling unprecedented performance improvements in AI applications. HyperAccel addresses these specific requirements with purpose-built silicon architecture.</p>"},{"location":"#llm-processing-unit-lpu","title":"LLM Processing Unit (LPU)","text":"<p>LPU is HyperAccel's breakthrough hardware architecture designed from the ground up for large language model inference. Unlike traditional accelerators that were originally designed for other computing tasks, the LPU is purpose-built to handle the specific computational patterns of LLMs.</p>"},{"location":"#key-lpu-advantages","title":"Key LPU Advantages","text":"<ul> <li>LLM-Specific Design: Hardware optimized for transformer architectures and attention mechanisms</li> <li>Latency-Optimized Architecture: Specifically designed to minimize inference latency for LLM workloads</li> <li>Energy Efficiency: Delivers superior performance-per-watt compared to traditional accelerators like NVIDIA GPUs</li> <li>High Scalability: Efficiently handles models of varying sizes and complexities  </li> </ul>"},{"location":"#hyperdex-software-stack","title":"HyperDex Software Stack","text":"<p>HyperDex is HyperAccel's comprehensive end-to-end (E2E) software solution that enables you to fully harness the power of LPU technology. It provides a complete ecosystem for running large language models efficiently on our specialized hardware. The toolchain ensures a smooth transition from existing GPU-based workflows to LPU-optimized inference without requiring significant code changes.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to accelerate your LLM inference? This documentation covers:</p> <ul> <li>Installation Guide: Set up HyperDex on your system</li> <li>Quick Start: Run your first model in minutes</li> </ul>"},{"location":"#support","title":"Support","text":"<p>Need help or have questions? Our team is here to support you every step of the way. Please contact us for assistance.</p>"},{"location":"_install_xrt/","title":"install xrt","text":"<p>To use the LPU effectively, installing the Xilinx Runtime (XRT) software is essential. XRT ensures smooth integration with Xilinx's Alveo FPGA, optimizing software-hardware communication for peak performance. It also supports the HyperDex Runtime Library (HRT) and ensures compatibility with Centos-7, Ubuntu 22.04 LTS, Rocky 8.4.</p> <p>Before starting the installation, make sure that the LPU is properly connected to the device. Then, refer to the step-by-step installation guide provided below. To proceed, you will need to download the necessary packages from the Xilinx website.</p>"},{"location":"_install_xrt/#step-1-install-xrt-library","title":"Step 1. Install XRT Library","text":"<p>Note</p> <p>To install XRT, ensure that the kernel version and kernel-headers version match. Additionally, if your operating system is Ubuntu and the kernel version is 5.15.0-41-generic or higher, you will need to downgrade the kernel. The recommended kernel version is 5.15.0-25.</p> <p>For Each-based systems (e.g. Centos-7, Ubuntu 22.04 LTS, Rocky 8.4)</p> <ul> <li>Centos-7, Rocky 8.4</li> </ul> <pre><code># yum install xrt_202310.2.15.225_8.1.1911-x86_64-xrt.rpm\n</code></pre> <ul> <li>Ubuntu 22.04 LTS</li> </ul> <p><pre><code># dpkg -i xrt_202310.2.15.225_22.04-amd64-xrt.deb\n</code></pre> Once the installation is complete, the XRT files will be located under the <code>/opt/xilinx/xrt/</code> directory. You will need to source the <code>/opt/xilinx/xrt/setup.sh</code> script to use commands such as <code>xbutil</code> and <code>xbmgmt</code>.</p> <pre><code>$ source /opt/xilinx/xrt/setup.sh\n</code></pre> How to use XRT Commands xbutil : Used primarily for device status monitoring and diagnostics. xbmgmt : Focuses on firmware management, flashing, and hardware-related tasks.  You can use <code>xbutil</code> and <code>xbmgmt</code> commands to examine device information. For more details, you can use the <code>--help</code> option with these commands or refer to the documentation on the XRT Master documentation.  <pre><code>$ xbutil examine\nSystem Configuration\n  OS Name              : Linux\n  Release              : 5.15.0-25-generic\n  Version              : #25-Ubuntu SMP Wed Mar 30 15:54:22 UTC 2022\n  Machine              : x86_64\n  CPU Cores            : 48\n  Memory               : 257574 MB\n  Distribution         : Ubuntu 22.04.4 LTS\n  GLIBC                : 2.35\n  Model                : ESC4000-E10\n\nXRT\n  Version              : 2.15.225\n  Branch               : 2023.1\n  Hash                 : adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  Hash Date            : 2023-05-03 10:13:19\n  XOCL                 : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  XCLMGMT              : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n\nDevices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:31:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n[0000:32:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  user(inst=---)  Yes\n</code></pre> <pre><code>$ xbmgmt examine\nSystem Configuration\n  OS Name              : Linux\n  Release              : 5.15.0-25-generic\n  Version              : #25-Ubuntu SMP Wed Mar 30 15:54:22 UTC 2022\n  Machine              : x86_64\n  CPU Cores            : 48\n  Memory               : 257574 MB\n  Distribution         : Ubuntu 22.04.4 LTS\n  GLIBC                : 2.35\n  Model                : ESC4000-E10\n\nXRT\n  Version              : 2.15.225\n  Branch               : 2023.1\n  Hash                 : adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  Hash Date            : 2023-05-03 10:13:19\n  XOCL                 : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n  XCLMGMT              : 2.15.225, adf27adb3cfadc6e4c41d6db814159f1329b24f3\n\nDevices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:31:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n[0000:32:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------------  mgmt(inst=---)  Yes\n</code></pre>"},{"location":"_install_xrt/#step-2-install-xrt-firmware","title":"Step 2. Install XRT Firmware","text":"<p>The next step is to install the XRT Firmware, which enables the FPGA to handle both hardware acceleration and general-purpose processing. To complete the installation, you need to flash the firmware onto the FPGA card. Follow the instructions provided with the firmware package and use the specified shell commands to deploy it onto the card.</p> <p>Download the Deployment Target Platform: This is the communication layer physically implemented and flashed into the card.</p> <p>For Each-based systems (e.g. Centos-7, Ubuntu 22.04 LTS, Rocky 8.4)</p> <ul> <li>Centos-7, Rocky 8.4</li> </ul> <pre><code># tar -xvf xilinx-u55c-gen3x16-xdma_2023.1_2023_0507_2220-noarch.rpm.tar.gz\n# yum install xilinx-*.rpm\n</code></pre> <ul> <li>Ubuntu 22.04 LTS</li> </ul> <pre><code># tar -xvf xilinx-u55c-gen3x16-xdma_2023.1_2023_0507_2220-all.deb.tar.gz\n# dpkg -i  xilinx-*.deb\n</code></pre> <p>Flash the Firmware: Execute the following command in the shell to flash the firmware onto the FPGA card: <pre><code># xbmgmt program --base --device &lt;BDF&gt; --image xilinx_u55c_gen3x16_xdma_base_3\n</code></pre></p>"},{"location":"_install_xrt/#step-3-cold-reboot","title":"Step 3. Cold Reboot","text":"<p>A cold reboot is required after installing the Xilinx firmware to ensure the system initializes and applies the updated firmware to the FPGA hardware.</p> <p>The FPGA may appear as two logical devices. This happens because the FPGA is operating in dual mode, with each logical device assigned to handle different tasks, such as hardware acceleration and general-purpose processing.</p> <p>After applying the shell, the Device Ready status is marked as \"Yes\". <pre><code>Devices present\nBDF             :  Shell                            Platform UUID        Device ID Device Ready*\n------------------------------------------------------------------------------------------------\n[0000:17:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------  user(inst=---)  Yes\n[0000:18:00.1]  :  xilinx_u55c_gen3x16_xdma_base_3  --------------  user(inst=---)  Yes\n</code></pre></p>"},{"location":"about_device/","title":"About Device","text":"<p>\ub3d9\uc77c 10B\ubaa8\ub378\uc5d0 \ub300\ud574 LPU / GPU \uc11c\ube59 \ub450 \ud658\uacbd\uc5d0\uc11c \uc0dd\uc131\uacb0\uacfc\uac00 \ucc28\uc774\uac00 \ub9ce\uc774 \ub098\ub294 \ubd80\ubd84\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>GPU/LPU \ud558\uc774\ube0c\ub9ac\ub4dc \uc11c\ubc84\uc758 \uacbd\uc6b0\uc5d0 GPU only\ub85c \ud588\uc744 \ub54c\uc640 100\ud37c\uc13c\ud2b8 \uc77c\uce58\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud558\ub4dc\uc6e8\uc5b4\ub9c8\ub2e4 \uc0ac\uc6a9\ud558\ub294 \uc5f0\uc0b0\uae30\uac00 \ub2e4\ub974\uace0, \uc5f0\uc0b0\uae30\uc5d0 \ub530\ub77c\uc11c float 16 \ub370\uc774\ud130 \uc911\uc5d0 \ud558\uc704 \ube44\ud2b8 \uc77c\ubd80\uac00 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\u00a0</p> <p>LPU\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \ud2b9\ubcc4\ud788 \uc81c\uc791\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc57c \ud558\ub098\uc694?</p> <p>\uc800\ud76c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ud1b5\uc6a9\ub418\ub294 huggingface format\uc758 model\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e4\ub9cc \uc800\ud76c \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \ub3d9\uc791\uc2dc\ud0a4\uae30 \uc704\ud574\uc11c\ub294 \ubaa8\ub378 \uc2e4\ud589 \uc774\uc804\uc5d0 \uc800\ud76c \ud558\ub4dc\uc6e8\uc5b4 \ud615\uc2dd\uc5d0 \ub9de\uac8c compile\ud558\ub294 \uacfc\uc815\uc774 \ud544\uc694\ud558\uba70, \uc774\ub294 \uc790\ub3d9\uc73c\ub85c \uc9c4\ud589\ub429\ub2c8\ub2e4..</p> <p>LPU \uc0c1\ud0dc\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub294 \uba85\ub839\uc5b4\uac00 \uc788\ub098\uc694?</p> <p>\uc790\uccb4 \uac1c\ubc1c\ud55c \uba85\ub839\uc5b4 \ubc0f FPGA \uba85\ub839\uc5b4 \uacf5\uc720 \ub4dc\ub9bd\ub2c8\ub2e4. <code>hyperdex-smi</code>\ub97c \uc0ac\uc6a9\ud558\uc5ec <code>nvidia-smi</code>\uc640 \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c LPU\uc758 \ub3d9\uc791 \uc0c1\ud0dc\ub97c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>hyperdex-reset</code>\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub514\ubc14\uc774\uc2a4\uac00 \ub370\ub4dc\ub77d \uc0c1\ud0dc\uc5d0 \ube60\uc84c\uc744 \ub54c LPU\ub97c \ucd08\uae30\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>xbutil examine</code>\uc744 \uc0ac\uc6a9\ud558\uc5ec LPU\uac00 \uc798 \uc7a5\ucc29\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Trouble-Shooting \ud398\uc774\uc9c0\ub97c \ucc38\uace0 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.</p>"},{"location":"about_server/","title":"About Server","text":"<p>\uc5e3\uc9c0 \uc11c\ubc84\uc758 \ub3d9\uc2dc\uc811\uc18d\uc790\ub294 \uba87\uba85\uae4c\uc9c0 \uac00\ub2a5\ud55c\uac00\uc694?</p> <p>TPOT, TTFT \ub4f1 \uc138\ubd80 \uc870\uac74\uc5d0 \ub530\ub77c, \ud558\ub4dc\uc6e8\uc5b4\ub97c \uc5b4\ub5bb\uac8c \uc138\ud305\ud558\ub0d0\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9c8 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \ud604\uc7ac edge \uc11c\ubc84\ub97c \uae30\uc900\uc73c\ub85c \ub9d0\uc500\ub4dc\ub9ac\uba74 \uc18c\uc218\uc758 \ub3d9\uc2dc\uc811\uc18d\uc790(\uc57d 4\uba85)\uae4c\uc9c0 \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4. \ucd94\ud6c4 \uc591\uc0b0\ub420 ASIC \uce69\uc740 256\uba85 \uc774\uc0c1\uc758 \uc0ac\uc6a9\uc790\uae4c\uc9c0 \ub3d9\uc2dc\uc5d0 \uc9c0\uc6d0 \uac00\ub2a5\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.</p> <p>\uc5e3\uc9c0\uc11c\ubc84 \ud558\ub4dc\uc6e8\uc5b4 \uc0ac\uc591\uc744 \uc54c\uace0 \uc2f6\uc2b5\ub2c8\ub2e4.</p> <p>\uc7a5\ucc29\ub41c \uac00\uc18d\uae30: U55C x 2, A10 x 1 CPU: Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz SSD SATA 512GB DRAM: 64GB</p> <p>\ub124\ud2b8\uc6cc\ud06c \uc18d\ub3c4\ub294 \uc5b4\ub290\uc815\ub3c4\ub97c \ud544\uc694\ub85c \ud558\ub098\uc694?</p> <p>\uc800\ud76c \uc11c\ubc84\ub294 \ub124\ud2b8\uc6cc\ud06c \uc5f0\uacb0 \uc5c6\uc774 on-premise\ub85c \ub3d9\uc791 \uac00\ub2a5\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub378\ub9cc \uc788\ub2e4\uba74 \ub124\ud2b8\uc6cc\ud06c \uc5c6\uc774 \ub3d9\uc791 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p> <p>\uc11c\ubc84\uc5d0 \ubb38\uc81c\uac00 \uc0dd\uacbc\uc744 \ub54c \uae34\uae09\ub300\uc751\uc808\ucc28\ub294 \uc5b4\ub5bb\uac8c \ub418\ub098\uc694?</p> <p>HyperAccel Service Desk\ub97c \uc6b4\uc601 \uc911\uc785\ub2c8\ub2e4. \uc720\ud615\uc5d0 \ub9de\uac8c \uc774\uc288 \uc81c\ucd9c\ud574\uc8fc\uc2dc\uba74, \ucd5c\ub300\ud55c \ube60\ub974\uac8c \ub300\uc751\ud574\ub4dc\ub9ac\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc9c0\uc6d0\ud558\ub294 OS \uc885\ub958</p> <p>OS\ub294 Centos-7, Ubuntu 22.04 LTS, Rocky 8.4 \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4.</p>"},{"location":"about_software/","title":"About Software","text":"<p>\uc9c0\uc6d0\uac00\ub2a5\ud55c \uc5b8\uc5b4\ubaa8\ub378 \uc0ac\uc774\uc988\uac00 \uc5b4\ub5bb\uac8c \ub418\ub098\uc694?</p> <p>8LPU\uc778 orion \uc11c\ubc84\uc758 \uacbd\uc6b0 \ucd5c\ub300 66B\uae4c\uc9c0 \uac00\ub2a5\ud569\ub2c8\ub2e4. </p> <p>\uc800\ud76c\uac00 \uae30\uc874\uc5d0 GPU \uae30\ubc18 \uc11c\ube59\uc5d0 \uc0ac\uc6a9\ud558\uace0\uc788\ub294 vLLM\uacfc \uc720\uc0ac\ud558\uac8c \uc2e4\uc2dc\uac04 \uc694\uccad\uc0ac\ud56d\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 LPU\uae30\ubc18\uc758 \uc11c\ube59 \ud504\ub808\uc784\uc6cc\ud06c\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ud234\uc774 \uc874\uc7ac\ud560\uae4c\uc694?</p> <p>\ub124, \uac00\ub2a5\ud569\ub2c8\ub2e4. LPU \ud658\uacbd\uc744 \uc9c0\uc6d0\ud558\ub294 vLLM plugin\uc744 \uc9c0\uc6d0\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \uc774\uc6a9\ud558\uc5ec vLLM\uacfc \ub3d9\uc77c\ud55c \uc778\ud130\ud398\uc774\uc2a4\ub85c \uc0ac\uc6a9\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ud55c\uad6d\uc5b4\ub3c4 \uc9c0\uc6d0 \uac00\ub2a5\ud55c\uac00\uc694?</p> <p>\ubaa8\ub378\uc774 \ud2b9\uc815 \uc5b8\uc5b4\ub97c \uc9c0\uc6d0\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc740 \ud558\ub4dc\uc6e8\uc5b4\uac00 \uc544\ub2c8\ub77c Training \ubc0f fine-tuning\uc758 \uc601\uc5ed\uc785\ub2c8\ub2e4. \uc800\ud76c LPU \uc5ed\uc2dc \ubaa8\ub378\uc774 \uc9c0\uc6d0\ud558\ub294 \uc5b8\uc5b4\uc640 \ubb34\uad00\ud558\uac8c \uc798 \ub3d9\uc791\ud569\ub2c8\ub2e4. HpyerClovaX, A.X \ub4f1 \ud55c\uad6d\uc5b4\ub97c \uc798\ud558\ub294 \ubaa8\ub378\uc744 \ub2e4\uc6b4\ubc1b\uc544 \uc2e4\uc81c \uc804\uc2dc\ud68c\uc5d0\uc11c \ud55c\uad6d\uc5b4\ub85c \ub370\ubaa8\ud55c \uacbd\ud5d8\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc9c0\uc6d0 \uac00\ub2a5\ud55c \ubaa8\ub378\uc758 \uc885\ub958</p> <p>Transformer Decoder \ud615\uc2dd\uc758 \ubaa8\ub378 \uc911 \ub2e4\uc591\ud55c architecture\ub97c \uc9c0\uc6d0 \uac00\ub2a5\ud569\ub2c8\ub2e4. (ex: Llama, Qwen2, Phi, ...) \uc774\uacf3\uc5d0\uc11c LPU\uc5d0\uc11c \uc9c0\uc6d0 \uac00\ub2a5\ud55c \ubaa8\ub378\ub4e4\uc744 \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc131\ub2a5\uc740 \ubaa8\ub378\uc5d0 \ub530\ub77c \uc57d\uac04\uc758 \ucc28\uc774\ub294 \uc788\uc73c\ub098 \uac70\uc758 \ubaa8\ub378\uc758 \ud06c\uae30\uc640 \ube44\ub840\ud569\ub2c8\ub2e4.</p> <p>python. transformer library\uc5d0\uc11c to(\u2019lpu\u2019)\ucc98\ub7fc \ub514\ubc14\uc774\uc2a4\ub97c \uc124\uc815\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218\ub294 \uc5c6\ub098\uc694?</p> <p>\uc800\ud76c\ub294 LPU\ub97c \uc0ac\uc6a9\uc790\uac00 \uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d python binding\uc744 transformer library\uc640 \uc720\uc0ac\ud558\uac8c \ub9cc\ub4e4\uc5b4 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc989 transformer library \ucf54\ub4dc\uc5d0 \uc800\ud76c \ud558\ub4dc\uc6e8\uc5b4\ub97c \ubd99\uc778 \uac83\uc774 \uc544\ub2c8\ub77c \uc790\uccb4 library\ub97c \ub9cc\ub4e0 \uac83\uc774\uae30\uc5d0 \uc0ac\uc6a9\ubc95\uc774 \uc644\uc804\ud788 \ub3d9\uc77c\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \u201cimport transformer\u201d \ub97c \uc0ac\uc6a9\ud558\uba74 \uae30\uc874\uc5d0 \uc0ac\uc6a9\ud558\uc2dc\ub358 transformer library\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\uace0, \u201cimport hyperdex.transformer\u201d\ub97c \uc0ac\uc6a9\ud558\uba74 \uc800\ud76c\uac00 \uc790\uccb4 \uac1c\ubc1c\ud55c HyperDex Library\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c model\uc744 \ubd80\ub974\uace0 generate\ub97c \uc218\ud589\ud558\ub294 \ubc29\uc2dd\uc740 \uc800\ud76c\uac00 \uc81c\uacf5\ud574\ub4dc\ub9b0 \uc608\uc2dc \ucf54\ub4dc\ub97c \ub530\ub77c\uc8fc\uc154\uc57c \uc815\uc0c1\uc791\ub3d9\ud569\ub2c8\ub2e4.</p> <p>HyperDex-Toolchain\uc5d0\uc11c \uc81c\uacf5\ud558\ub294\u00a0LPU\ub97c \uc0ac\uc6a9\ud558\ub824\uba74,\u00a0\uc5b8\uc5b4\ubaa8\ub378 \ucd08\uae30\ubd80\ud130, HyperDex-Toolchain\uc5d0\uc11c \uc81c\uacf5\ud558\ub294\u00a0SDK library\ub97c \uc774\uc6a9\ud558\uc5ec \uac1c\ubc1c\uc744 \ud574\uc57c\ud558\ub098\uc694?</p> <p>\uc544\ub2d9\ub2c8\ub2e4. HyperDex-Toolchain\ub294 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud558\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\uac00 \uc544\ub2cc \uc774\ubbf8 \uac1c\ubc1c\ub41c \ubaa8\ub378\uc744\u00a0LPU\ub85c \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac \uc785\ub2c8\ub2e4.\u00a0\ub3c5\ub9bd\uc801\uc73c\ub85c \uac1c\ubc1c\ud558\uc2e0 \ubaa8\ub378\ub3c4\u00a0http://huggingface.co \u00a0\uc5d0 \uc62c\ub77c\uc640\uc788\ub2e4\uba74\u00a0LPU\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\u00a0\uc2e4\uc81c \ub2e4\ub978 \ud68c\uc0ac\uc5d0\uc11c\u00a0fine-tuning\u00a0\ud55c \ubaa8\ub378\uc740 \ubb3c\ub860\u00a0foundation model\uc744 \uac1c\ubc1c\ud55c \ubaa8\ub378\uae4c\uc9c0 \uc218 \ucc28\ub840\u00a0PoC\u00a0\uc9c4\ud589\ud55c \uacbd\ud5d8\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uae30\uc874 \ucf54\ub4dc(\uc608\ub97c \ub4e4\uc5b4\u00a0transformer\uc744 \uc774\uc6a9\ud55c)\ub97c\u00a0hyperdex.transformer\u00a0\ub85c \ubcc0\uacbd\ud558\uba74, LPU\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub098\uc694?</p> <p>\ub9de\uc2b5\ub2c8\ub2e4.\u00a0\uc774\ubbf8 \uc774\ud574\ud558\uc2e0 \uac83 \uac19\uc9c0\ub9cc \ub2e4\uc2dc \ub9d0\uc500\ub4dc\ub9ac\uba74 \ubc31\uc5d4\ub4dc\ub97c \uc644\uc804\ud558\uac8c \ubd99\uc778 \uac83\uc774 \uc544\ub2c8\ub77c \uae30\uc874 \uc624\ud508\uc18c\uc2a4\ub97c \ubaa8\ubc29\ud55c \uac83\uc774\ubbc0\ub85c \ubb38\ubc95\uc740 \uc800\ud76c \uc608\uc2dc \ucf54\ub4dc\ub97c \ucc38\uace0\ud574\uc8fc\uc154\uc57c \ub354 \uc798 \ub3d9\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>LPU\ub97c \ud14c\uc2a4\ud2b8\ud558\uae30 \uc704\ud574\uc11c\ub294 HyperDex-Toolchain\uc6a9\uc73c\ub85c\u00a0code\ub97c \ubcc0\ud658\ud558\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud558\uace0, hyperdex\ub97c \uacf5\ubd80\ud574\uc57c \ud558\ub294 \uacfc\uc815\ub3c4 \ud544\uc694\ud55c\uac00\uc694?</p> <p>\uc800\ud76c\uac00\u00a0python package\ub85c \ud3ec\uc7a5\uc744 \ub2e4 \ud574\ub450\uc5c8\uae30\uc5d0\u00a0generate()\u00a0\ud568\uc218\ub97c \uc2e4\ud589\uc2dc\ud0a4\ub294 \ubd80\ubd84\ub9cc \uc218\uc815\ud574\uc8fc\uc2dc\uba74 \ub3d9\uc791\ud569\ub2c8\ub2e4.\u00a0\ub530\ub77c\uc11c \uba87 \uc904\ub9cc \uac04\ub2e8\ud788 \uc218\uc815\ud558\uba74 \ub2e4\ub978 \ucf54\ub4dc\ub4e4\uacfc \uc798 \ub9de\ubb3c\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\u00a0\ud639\uc2dc \uc5b4\ub824\uc6c0\uc774 \uc788\ub2e4\uba74 \uae30\uc220\uc9c0\uc6d0\uc744 \ucd94\uac00\ub85c \ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>HyperDex-Toolchain\ub97c \uc124\uce58\ud558\ub824\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c \ud558\ub294 \uc9c0 \ud655\uc778\ubd80\ud0c1\ud569\ub2c8\ub2e4.\u00a0pip install hyperdex-toolchain\u00a0\ud558\ub2c8 \uc548\ub418\ub124\uc694.</p> <p>\ud604\uc7ac\u00a0hyperdex library\ub294 \uc624\ud508 \uc18c\uc2a4\uac00 \uc544\ub2c8\ubbc0\ub85c \uc800\ud76c\uac00\u00a0licensing\u00a0\ud6c4 https://pypi.hyperaccel.ai \ub97c \ud1b5\ud574 \ubc30\ud3ec\ud574\ub4dc\ub9ac\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uacc4\uc815 \uc0dd\uc131 \uc774\ud6c4 \uc0ac\uc6a9\uac00\ub2a5 \ud558\ubbc0\ub85c, \uc138\uc77c\uc988 \ud300\uc5d0 \ubb38\uc758 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.</p>"},{"location":"chat_ui/","title":"Chat UI","text":""},{"location":"chat_ui/#hyperaccel-chatbot-with-orion-server","title":"HyperAccel Chatbot with Orion Server","text":"<p>HyperAccel supports a ChatGPT-style chatbot demo. If you are interested in the demo, please contact us</p> <p></p>"},{"location":"huggingface_api/","title":"HuggingFace API","text":"<p>HyperDex provides a Python API designed to make running workloads on the LPU both easy and efficient. The Python API uses function calls similar to those found in HuggingFace\u2019s transformers library, allowing users familiar with HuggingFace to quickly adapt to and utilize the HyperDex system. This enables existing HuggingFace users to seamlessly integrate HyperDex into their workflows without a steep learning curve.\u200b</p>"},{"location":"huggingface_api/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS, Rocky 8.4</li> <li>Python: 3.10 ~ 3.12</li> <li>Xilinx Runtime Library</li> </ul>"},{"location":"huggingface_api/#install-with-pip","title":"Install with pip","text":"<p>You can install <code>hyperdex-toolchain</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new environemnt.\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n$ uv venv -p python==3.10 --no-project --seed .hdex-env\n$ source .hdex-env/bin/activate\n\n$ # Install HyperDex-Python\n$ uv pip install -i https://&lt;pypi_id:pypi_pw&gt;@pypi.hyperaccel.ai/simple hyperdex-toolchain==1.5.2+cpu\n</code></pre>"},{"location":"huggingface_api/#text-generation-with-hyperaccel-lputm","title":"Text Generation with HyperAccel LPU\u2122","text":"<p>HyperDex allows you to generate output tokens using a function similar to HuggingFace's <code>generate</code> function. Therefore, you can easily generate tokens as shown in the example below.</p> <p>Tip</p> <p>You may need to log in to Hugging Face Hub.</p> <pre><code>import os\nfrom hyperdex.tools import AutoCompiler\nfrom hyperdex.transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Path to hyperdex checkpoint (MODIFY model path and model here)\nmodel_id = {MODEL_ID_YOU_WANT}\n\n# MODIFY hardware configuration here\nlpu_device = 2\ngpu_device = 0\n\ndef main():\n    hdex = AutoCompiler()\n    hdex.compile(\n        model_id=model_id,\n        num_device=lpu_device,\n        max_length=4096,\n        low_memory_usage=False,\n        fast_compile=True,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id=model_id,\n        device_map={\"gpu\": gpu_device, \"lpu\": lpu_device}\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\n    # Input text (MODIFY your input here)\n    inputs = \"Hello, my name\"\n\n    # 1. Encode input text to input token ids\n    input_ids = tokenizer.encode(inputs)\n\n    # 2. Generate output token ids with LPU\u2122 (MODIFY sampling parameters here)\n    output_ids = model.generate(\n        input_ids,\n        max_new_tokens=512,\n        # Sampling\n        do_sample=True,\n        top_p=0.7,\n        top_k=1,\n        temperature=1.0,\n        repetition_penalty=1.2\n    )\n\n    # 3. Decode output token ids to output text\n    outputs = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n    # Print the output context\n    print(outputs)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"huggingface_api/#lpu-gpu-hybrid-system","title":"LPU-GPU Hybrid System","text":"<p>Starting from version 1.3.2, HyperDex-Python supports the LPU-GPU hybrid system. The GPU, which has relatively higher computing power, handles the Prefill part of the Transformer, while the LPU, which efficiently utilizes memory bandwidth, processes the Decode part. The Key-Value transfer between Prefill and Decode can be performed without overhead using HyperDex's proprietary technology. You can select the number of devices to use for both GPU and LPU through the <code>device_map</code> option.</p> <p>Note</p> <p>To run the <code>LPU-GPU hybrid system</code>, you need to have <code>CUDA 12.6</code> installed on your system. Additionally, since the GPU utilizes <code>PyTorch</code> to run LLMs, it is recommended to install <code>torch version 2.7.0 or later</code> to ensure optimal compatibility and performance.\u200b</p> <pre><code>import os\nfrom hyperdex.tools import AutoCompiler\nfrom hyperdex.transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Path to hyperdex checkpoint (MODIFY model path and model here)\nmodel_id = \"facebook/opt-1.3b\"\n\n# MODIFY hardware configuration here\nlpu_device = 2\ngpu_device = 2\n\ndef main():\n    hdex = AutoCompiler()\n    hdex.compile(\n        model_id=model_id,\n        num_device=lpu_device,\n        max_length=4096,\n        low_memory_usage=False,\n        fast_compile=True,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id=model_id,\n        device_map={\"gpu\": gpu_device, \"lpu\": lpu_device}\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\n    # Input text (MODIFY your input here)\n    inputs = \"Hello, my name\"\n\n    # 1. Encode input text to input token ids\n    input_ids = tokenizer.encode(inputs)\n\n    # 2. Generate output token ids with LPU\u2122 (MODIFY sampling parameters here)\n    output_ids = model.generate(\n        input_ids,\n        max_new_tokens=512,\n        # Sampling\n        do_sample=True,\n        top_p=0.7,\n        top_k=1,\n        temperature=1.0,\n        repetition_penalty=1.2\n    )\n\n    # 3. Decode output token ids to output text\n    outputs = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n    # Print the output context\n    print(outputs)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"huggingface_api/#sampling","title":"Sampling","text":"<p>Sampling works in the same way as HuggingFace. For sampling, you have options like top_p, top_k, temperature, and repetition penalty. Please refer to the HuggingFace documentation for explanations of each option. Additionally, the <code>generate</code> function allows you to directly control randomness using the seed argument. If <code>do_smaple</code> is <code>False</code>, LPU does not perform sampling and uses greedy method.</p> Sampling Arguments Description <code>top_p</code> Top-P sampling. Default value is <code>0.7</code> <code>top_k</code> Top-K sampling. Default value is <code>1</code> <code>temperature</code> Smoothing the logit distribution. Defualt value is <code>1.0</code> <code>repetition_penalty</code> Give penlaty to logits. Default value is <code>1.2</code> <code>stop</code> Token ID that signals the end of generation. Default value is <code>eos_token_id</code>"},{"location":"huggingface_api/#streaming-token-generation","title":"Streaming Token Generation","text":"<p>HyperDex supports streaming token generation in a similar manner to HuggingFace. You can activate it by passing the TextStreamer module as an argument to the <code>generate</code> function.</p> <pre><code># Import HyperDex transformers\nfrom hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\nfrom hyperdex.transformers import TextStreamer\n\n# Path to hyperdex model\nhyperdex_model = \"facebook/opt-1.3b\"\n\n# Load tokenzier and model\ntokenizer = AutoTokenizer.from_pretrained(model_id=hyperdex_model)\nmodel = AutoModelForCausalLM.from_pretrained(model_id=hyperdex_model, device_map={\"lpu\": 1})\n# Config streamer module\nstreamer = TextStreamer(tokenizer, skip_special_tokens=True)\n</code></pre> <p>Since the <code>TextStreamer</code> module includes the process of decoding through the <code>tokenizer</code> and printing internally, you only need to call the <code>generate</code> function.</p> <pre><code># Input text\ninputs = \"Hello world!\"\n\n# 1. Encode input text to input token ids\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\n# 2. Generate streaming output token ids with LPU\u2122\nmodel.generate(\n  input_ids,\n  max_length=1024,\n  # Sampling\n  do_sample=True,\n  top_p=0.7,\n  top_k=50,\n  temperature=1.0,\n  repetition_penalty=1.2\n)\n</code></pre>"},{"location":"huggingface_api/#how-to-use-streaming-with-other-application","title":"How to use streaming with other Application?","text":"<p>HyperDex utilizes the <code>yield</code> keyword in Python to enable streaming for use in other applications. When you call the <code>generate_yield</code> function, it returns using <code>yield</code>, making it easy to use in other Python applications.</p> <pre><code># Config streamer module with disable print to activate yield mode\nstreamer = TextStreamer(tokenizer, use_sse=True, use_print=False, skip_special_tokens=True)\n\n# Input text\ninputs = \"Hello world!\"\n\n# 1. Encode input text to input token ids\ninput_ids = tokenizer.encode(inputs, return_tensors=\"np\")\n# 2. Generate streaming output token ids with LPU\u2122\noutput_ids = model.generate_yield(\n  input_ids,\n  max_length=1024,\n  # Sampling\n  do_sample=True,\n  top_p=0.7,\n  top_k=50,\n  temperature=1.0,\n  repetition_penalty=1.2,\n  # Streaming\n  streamer=streamer\n)\n\n# Stream output tokens in real-time\nfor token, is_end in next(output_ids):\n    if is_end:\n        break\n    decoded_text = tokenizer.decode(token, skip_special_tokens=True)\n    print(decoded_text, end='', flush=True)\n\n# Use outputs_ids which type is generator\n</code></pre>"},{"location":"hyperdex_toolchain/","title":"HyperDex Toolchain","text":"<p>The HyperDex Toolchain is a comprehensive software stack that simplifies the deployment and execution of AI workloads on LPU (Language Processing Unit) hardware, ensuring optimal performance. It includes device drivers, runtime environment, and compiler tools that automate all processes required to initialize, compile, and execute models on the LPU.</p> <p>This toolchain is delivered as a single, integrated package comprising two core components: <code>hyperdex.tools</code> and <code>hyperdex.transformers</code>, which serve as the compiler and runtime packages respectively. Together, they enable developers to efficiently deploy LLMs on LPU hardware with minimal modifications to existing workflows.  </p>"},{"location":"hyperdex_toolchain/#hyperdextools-compiler","title":"hyperdex.tools \u2014 Compiler","text":"<p><code>hyperdex.tools</code> is a Python-based compiler interface specifically designed to support HuggingFace Transformers models. It provides a high-level entry point for seamlessly adapting pre-trained models to LPU hardware architecture.</p> <p>The core of this component consists of two primary classes: AutoCompiler and AutoModelConverter.</p> <ul> <li>AutoCompiler orchestrates the entire compilation pipeline by verifying model compatibility with LPU hardware, downloading the target model, and preparing it for execution using LPU-optimized instructions.</li> <li>AutoModelConverter performs comprehensive validation of downloaded checkpoints, analyzes both the model architecture and tokenizer configuration, and converts them into an LPU-compatible format.</li> </ul> <p>Through automated handling of these complex processes, <code>hyperdex.tools</code> enables developers to seamlessly migrate HuggingFace models to LPU hardware with minimal manual intervention.  </p>"},{"location":"hyperdex_toolchain/#hyperdextransformers-runtime","title":"hyperdex.transformers \u2014 Runtime","text":"<p><code>hyperdex.transformers</code> provides the runtime environment for executing LLMs on the LPU. It is fully aligned with the HuggingFace Transformers interface, enabling developers to leverage familiar APIs while benefiting from LPU acceleration.</p> <p>Key functionality includes:</p> <ul> <li>AutoConfig - Loads runtime parameters such as the number of devices and maximum token length</li> <li>AutoModelForCausalLM - Loads and executes compiled LPU models</li> <li>AutoTokenizer - Retrieves the appropriate tokenizer for each model</li> <li>TextStreamer - Streams decoded tokens incrementally in real-time, either to stdout or via Server-Sent Events (SSE), enabling words to appear as they are generated</li> </ul> <p>Additionally, the runtime supports prefix caching, which allows repeated or long-running workloads to be processed more efficiently.  </p>"},{"location":"hyperdex_toolchain/#compatibility","title":"Compatibility","text":"<p>All classes and tools within the HyperDex Toolchain are compatible with HuggingFace's <code>transformers</code> library. This ensures that developers can leverage the same ecosystem they are familiar with while seamlessly transitioning their workloads to LPU-based execution.</p> <p>Before using the HyperDex Toolchain, please ensure you have met all the requirements listed in the Prerequisites section.</p>"},{"location":"hyperdex_toolchain/#prerequisites","title":"Prerequisites","text":"<ul> <li>An LPU device is installed and recognized (<code>lspci | grep -i xilinx</code>)</li> <li>XRT is installed successfully</li> <li>Linux kernel version is supported by both XRT and HyperDex Toolchain</li> <li>You have the necessary credentials to download required packages</li> </ul> <p>For installation instructions, please refer to HyperDex Toolchain Installation. If you prefer to use vLLM, please see vLLM Installation.</p>"},{"location":"install_hyperdex/","title":"HyperDex Installation","text":""},{"location":"install_hyperdex/#hyperdex-installation-guide","title":"HyperDex Installation Guide","text":"<p>This guide will walk you through installing HyperDex-Toolchain to run large language models on HyperAccel's LPU hardware.</p>"},{"location":"install_hyperdex/#overview","title":"Overview","text":"<p>The LPU is built on AMD's Alveo FPGA platform and requires two main components:</p> <ol> <li>Xilinx Runtime (XRT): Low-level drivers and runtime for FPGA hardware</li> <li>HyperDex-Toolchain: HyperAccel's optimized software stack for LLM inference</li> </ol> <p>The HyperDex-Toolchain includes a runtime package that allows the LPU to fully utilize its hardware capabilities, providing enhanced performance for large language models.</p>"},{"location":"install_hyperdex/#prerequisites","title":"Prerequisites","text":""},{"location":"install_hyperdex/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: RHEL-8/8 or Ubuntu-22.04-LTS</li> <li>Hardware: AMD Alveo FPGA card (U55C)</li> <li>Xilinx Runtime (XRT): Required for FPGA operations</li> </ul> <p>XRT Installation</p> <p>Before proceeding, make sure XRT is properly installed. Follow our XRT install guide for detailed instructions.</p>"},{"location":"install_hyperdex/#software-compatibility","title":"Software Compatibility","text":"<p>The table below shows the supported Python, CUDA, and PyTorch versions. Ensure your environment matches one of these configurations:</p> Python Version CUDA Version PyTorch Version 3.10, 3.11, 3.12 CPU-only, 12.6 2.7.0+cpu, 2.7.0+cu126"},{"location":"install_hyperdex/#step-1-set-up-python-environment","title":"Step 1: Set Up Python Environment","text":"<p>We recommend using <code>uv</code>, a fast Python package and virtual environment manager, to set up your environment quickly and efficiently.</p>"},{"location":"install_hyperdex/#install-uv-and-create-virtual-environment","title":"Install uv and Create Virtual Environment","text":"<pre><code># Install uv package manager\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a new virtual environment with Python 3.10\n$ uv venv -p python==3.10 --no-project --seed .hdex-env\n\n# Activate the virtual environment\n$ source .hdex-env/bin/activate\n</code></pre>"},{"location":"install_hyperdex/#step-2-install-hyperdex-python-package","title":"Step 2: Install HyperDex Python Package","text":"<p>HyperDex-Toolchain is distributed through HyperAccel's private PyPI server, which requires authentication credentials.</p>"},{"location":"install_hyperdex/#set-up-pypi-authentication","title":"Set Up PyPI Authentication","text":"<p>Environment Variables Required</p> <p>To run the following commands, you'll need your HyperAccel PyPI credentials. Please contact our support team if you need help obtaining credentials. <pre><code>export PYPI_ID=\"your_pypi_username\"\nexport PYPI_PW=\"your_pypi_password\"\n</code></pre> These credentials will be used to authenticate with HyperAccel's private PyPI server.</p>"},{"location":"install_hyperdex/#choose-your-installation","title":"Choose Your Installation","text":"<p>Select the appropriate installation option based on your hardware setup:</p>"},{"location":"install_hyperdex/#option-a-lpu-only-environment-cpu-pytorch","title":"Option A: LPU-Only Environment (CPU PyTorch)","text":"<pre><code># Install PyTorch CPU version\n$ uv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cpu\n\n# Install HyperDex-Toolchain for CPU\n$ uv pip install -i https://pypi.hyperaccel.ai/simple hyperdex-toolchain==1.5.2+cpu\n</code></pre>"},{"location":"install_hyperdex/#option-b-lpugpu-environment-cuda-pytorch","title":"Option B: LPU+GPU Environment (CUDA PyTorch)","text":"<pre><code># Install PyTorch CUDA version\n$ uv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Install HyperDex-Toolchain for CUDA\n$ uv pip install -i https://${PYPI_ID}:${PYPI_PW}@pypi.hyperaccel.ai/simple hyperdex-toolchain==1.5.2+cu126\n</code></pre>"},{"location":"install_hyperdex/#step-3-verification-and-next-steps","title":"Step 3: Verification and Next Steps","text":"<p>To operate the LPU successfully, ensure both components are properly installed:</p> <ol> <li>\u2705 Xilinx Runtime (XRT) - Hardware drivers and runtime</li> <li>\u2705 HyperDex-Toolchain - Python package and dependencies</li> </ol> <p>Verify Installation</p> <p>Use the following commands to confirm your installation is working correctly:</p> <p>Check XRT Installation: <pre><code># Verify XRT installation and hardware detection\n$ xbutil examine\n</code></pre></p> <p>Check HyperDex-Toolchain Installation: <pre><code># Verify HyperDex-Toolchain package is installed\n$ uv pip list | grep hyperdex-toolchain\n</code></pre></p>"},{"location":"install_hyperdex/#ready-to-start","title":"Ready to Start?","text":"<p>Once installation is complete and verified, you can begin using HyperDex for LLM inference. Proceed to our Quick Start Guide to run your first model on the LPU.</p>"},{"location":"offline_inference/","title":"vLLM API","text":"<p>HyperDex supports the vLLM framework to run on LPU. As you know, the vLLM framework officially supports a variety of hardware including GPU, TPU, and XPU. HyperDex has its own branch of vLLM with a backend specifically designed for LPU, making it very easy to use. If your system is already using vLLM, you can switch hardware from GPU to LPU without changing any code.</p>"},{"location":"offline_inference/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS, Rocky 8.4</li> <li>Python: 3.10 ~ 3.12</li> <li>torch: 2.7.0+cpu (in LPU only env) or 2.7.0+cu126 (in LPU+GPU env)</li> <li>Xilinx Runtime Library</li> <li>HyperDex-Toolchain</li> </ul>"},{"location":"offline_inference/#install-with-pip","title":"Install with pip","text":"<p>You can install <code>hyperdex-vllm</code> using pip, which requires access rights to HyperAccel's private PyPI server. To install the HyperDex Python package, run the following command:</p> <pre><code>$ # (Recommended) Create a new virtual environemnt. We recommend to use uv(https://docs.astral.sh/uv/)\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n$ uv venv -p python==3.10 --no-project --seed .hdex-env\n$ source .hdex-env/bin/activate\n\n$ # Register HyperAccel PyPI credentials. Please [contact our support team](mailto:support@hyperaccel.ai) if you need help obtaining credentials.\n$ export PYPI_ID=\"your_pypi_username\"\n$ export PYPI_PW=\"your_pypi_password\"\n\n$ # Install HyperDex-Toolchain and vLLM in LPU only env\n$ uv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cpu\n$ uv pip install -i https://${PYPI_ID}:${PYPI_PW}@pypi.hyperaccel.ai/simple vllm-orion==0.9.0+orion.toolchain152.fpga\n\n$ # Install HyperDex-Toolchain and vLLM in LPU+GPU env\n$ uv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126\n$ uv pip install -i https://${PYPI_ID}:${PYPI_PW}@pypi.hyperaccel.ai/simple vllm-orion==0.9.0+orion.toolchain152.hybrid\n</code></pre>"},{"location":"offline_inference/#text-generation-with-hyperaccel-lputm","title":"Text Generation with HyperAccel LPU\u2122","text":"<p>HyperDex-vLLM generates tokens very similar to vLLM's <code>generate</code> function, enabling you to easily generate tokens as demonstrated in the example below. to use LPU, you must give this environment variable \"NUM_LPU_DEVICES\" with number of devices to use</p> <p>Note</p> <p>currently, LPU supports one batch at a time. do not put more than one prompt.  multi-batch will be supported soon</p> <pre><code># Quick example for LPU execution in vLLM framework, content of \"main.py\"\nfrom vllm import LLM, SamplingParams\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Create prompts and a sampling params object.\nprompts = [\"Hello, my name is\"]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, top_k=1, min_tokens=30, max_tokens=30)\n\n# Create an LLM\nllm = LLM(model=\"/path/to/llama-7b\")\n\n# Generate texts from the prompts. \noutputs = llm.generate(prompts, sampling_params)\n\n# Log the outputs\nlogger.info(\"-\" * 60)\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    logger.info(f\"Prompt:    {prompt!r}\")\n    logger.info(f\"Output:    {generated_text!r}\")\n    logger.info(\"-\" * 60)\n</code></pre> <pre><code># run the code with command \nNUM_LPU_DEVICES=2 uv run vllm_lpu_model_runner.py\n</code></pre>"},{"location":"offline_inference/#gpu-lpu-hybrid-system","title":"GPU-LPU Hybrid System","text":"<p>HyperDex-Toolchain supports a heterogeneous GPU-LPU hardware system for executing large language models (LLM). Each hardware type offers distinct strengths: GPU excels in large-scale parallel computations, while LPU is designed to fully optimize memory bandwidth utilization. </p> <p>Since the prefill stage is compute-bound and the decode stage is memory-bound, the hybrid system processes prefill stage using a GPU, and decode stage using a LPU. This approach sighificantly boosts LLM performance!</p> <p>To enable the hybrid system, simply add NUM_GPU_DEVICES=2 at command.</p> <pre><code># run with this command\nNUM_GPU_DEVICES=2 NUM_LPU_DEVICES=2 uv run vllm_lpu_model_runner.py\n</code></pre>"},{"location":"offline_inference/#option-for-sampling-params","title":"Option for Sampling Params","text":"<p>Sampling Params refer to setting that control how a model generates text. vLLM supports various sampling params to support various features. However, due to the current limitations of the LPU, only the sampling parameters listed below are supported. We are planning to extend and update our coverage.</p> Sampling Arguments Description <code>max_tokens</code> The number of tokens to be generated. Default value is <code>16</code> <code>top_p</code> Top-P sampling. Default value is <code>0.7</code> <code>top_k</code> Top-K sampling. Default value is <code>1</code> <code>temperature</code> Smoothing the logit distribution. Defualt value is <code>1.0</code> <code>repetition_penalty</code> Give penlaty to logits. Default value is <code>1.2</code>"},{"location":"offline_inference/#option-for-llm-engine","title":"Option for LLM Engine","text":"<p>LLM Engine is a core component of vLLM because it performs various functions. For example, it initializes hardware, manages hardware resources, and schedules requests.</p> LLM Engine Arguments Description <code>model</code> Name of path of the huggingface model to use. Default: \"facebook/opt-125m\" <code>device</code> Device type for vLLM execution. Default: \"cuda\" <code>tokenizer</code> Name of path of the huggingface tokenizer to use. If not specified, model name of path will be used <code>trust-remote-code</code> Trust remote code from huggingface. Default: False"},{"location":"online_inference/","title":"vLLM Server","text":""},{"location":"online_inference/#requirements-and-install-guide","title":"Requirements and Install Guide","text":"<p>Requirements and Install Guide is same as vLLM API. </p>"},{"location":"online_inference/#serving-model","title":"Serving Model","text":"<p>hyperdex-vllm provides an HTTP server that implements vLLM API.  You can execute the server by the command below.</p> <pre><code>$ NUM_LPU_DEVICES=1 python -m vllm.entrypoints.api_server --model  facebook/opt-1.3b\n\n... OMISSION ...\n\nINFO:     Started server process [27157]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"online_inference/#descriptions-of-hyperdex-vllm-serve-arguments","title":"Descriptions of HyperDex-vLLM Serve Arguments","text":"<p>Arguments are the same as vLLM Engine.</p>"},{"location":"online_inference/#client","title":"Client","text":"<p>To call the server, run this example command at another terminal</p> <pre><code>curl -X POST \"http://localhost:8000/generate\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"prompt\": \"Hello, my name is\",\n      \"max_tokens\": 30,\n      \"temperature\": 0\n  }'\n</code></pre>"},{"location":"online_inference/#openai-client","title":"OpenAI Client","text":""},{"location":"online_inference/#serving-model_1","title":"Serving Model","text":"<p>Our vLLM also provides an HTTP server that implements OpenAI's Completions API. You can call the server by the command below.</p> <pre><code>$ NUM_GPU_DEVICES=1 NUM_LPU_DEVICES=2 vllm serve facebook/opt-1.3b\n\n... OMISSION ...\n\nINFO:     Started server process [27157]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"online_inference/#client_1","title":"Client","text":"<p>To call the server, you can use OpenAI Python client library, or any other HTTP client.</p> <pre><code>curl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the ^Crld Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n  }'\n</code></pre>"},{"location":"quick_start/","title":"Quick Start Guide","text":"<p>Get started with HyperDex in just a few minutes! This guide shows you how to run your first large language model inference on HyperAccel's LPU.</p>"},{"location":"quick_start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>\u2705 Xilinx Runtime (XRT) installed on your system</li> <li>\u2705 HyperDex-Toolchain Python package installed</li> <li>\ud83d\udcdd If you haven't completed the installation, follow our Installation Guide</li> </ul>"},{"location":"quick_start/#your-first-llm-inference","title":"Your First LLM Inference","text":""},{"location":"quick_start/#step-1-import-hyperdex-libraries","title":"Step 1: Import HyperDex Libraries","text":"<p>HyperDex provides a familiar API similar to HuggingFace Transformers, making it easy to get started.</p> <pre><code>from hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\n</code></pre>"},{"location":"quick_start/#step-2-load-model-and-tokenizer","title":"Step 2: Load Model and Tokenizer","text":"<pre><code># Load tokenizer from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n\n# Load model and map it to LPU device\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", device_map={\"lpu\": 1})\n</code></pre> <p>Device Mapping</p> <p>The <code>device_map={\"lpu\": 1}</code> parameter tells HyperDex to load the model onto the LPU hardware.</p>"},{"location":"quick_start/#step-3-generate-text","title":"Step 3: Generate Text","text":"<pre><code># Encode input text\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\n\n# Generate text using the LPU\noutput_ids = model.generate(input_ids, max_length=1024, do_sample=False)\n\n# Decode the generated output\noutputs = tokenizer.decode(output_ids)\nprint(outputs)\n</code></pre>"},{"location":"quick_start/#complete-example","title":"Complete Example","text":"<p>Here's the complete working example:</p> <pre><code>from hyperdex.transformers import AutoModelForCausalLM\nfrom hyperdex.transformers import AutoTokenizer\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", device_map={\"lpu\": 1})\n\n# Generate text\ninput_ids = tokenizer.encode(\"Hello world!\", return_tensors=\"np\")\noutput_ids = model.generate(input_ids, max_length=1024, do_sample=False)\noutputs = tokenizer.decode(output_ids)\nprint(outputs)\n</code></pre>"},{"location":"quick_start/#key-design-principles","title":"Key Design Principles","text":"<ul> <li>Familiar APIs: HyperDex mirrors HuggingFace Transformers APIs, so you can leverage your existing knowledge</li> <li>Hardware Abstraction: LPU optimization schemes are handled automatically behind the scenes</li> <li>Zero Learning Curve: If you know HuggingFace, you already know HyperDex</li> </ul>"},{"location":"quick_start/#next-steps","title":"Next Steps","text":"<p>\ud83c\udfaf Ready to explore more? Check out these resources:</p> <ul> <li>Advanced Examples: vLLM API for LPU</li> <li>API Documentation: Detailed API reference</li> </ul>"},{"location":"quick_start/#additional-resources","title":"Additional Resources","text":"<p>\ud83d\udcd6 Quick Reference Guide: If you have a server with HyperDex already installed, you can also refer to our PDF user guide for a quick overview.</p>"},{"location":"supported_models/","title":"Supported Models","text":"<p>HyperDex supports a variety of generative Transformer models in HuggingFace Transformers. The following is the list of model architectures that are currently supported by HyperDex. Alongside each architecture, we include some popular models that use it.</p> Architecture Models Example HuggingFace Models Orion Hybrid <code>CohereForCausalLM</code> Cohere <code>CohereForAI/c4ai-command-r-v01</code>, etc. <code>ExaoneForCausalLM</code> EXAONE <code>LGAI-EXAONE/EXAONE-3.0-7.8B</code>, etc. <code>FalconForCausalLM</code> Falcon <code>tiiuae/falcon-7b</code>, <code>tiiuae/falcon-40b</code>, etc. <code>GemmaForCausalLM</code> Gemma <code>google/gemma-2b</code>, <code>google/gemma-7b</code>, etc. <code>GPT2LMHeadModel</code> GPT-2 <code>gpt2</code>, <code>gpt2-xl</code>, etc. <code>GPTBigCodeForCausalLM</code> StarCoder, SantaCoder, WizardCoder <code>bidcode/starcoder</code>, etc. <code>GPTJForCausalLM</code> GPT-J <code>EleutherAI/gpt-j-6b</code>, etc. <code>GPTNeoXForCausalLM</code> GPT-NeoX,Pythia,StableLM <code>EleutherAI/gpt-neox-20b</code>, <code>EleutherAI/pythia-12b</code>, etc. <code>LlamaForCausalLM</code> Llama,Lllama-2,Llama-3,Midm <code>meta-llama/Llama-2-7b-hf</code>, <code>K-intelligence/Midm-2.0-Base-Instruct</code>, etc. <code>MistralForCausalLM</code> MistralMistral-Instruct <code>mistralai/Mistral-7B-v0.1</code>, etc. <code>Qwen2ForCausalLM</code> Qwen2,Qwen2.5,A.X-4.0 <code>Qwen/Qwen2.5-7B-Instruct</code>,<code>skt/A.X-4.0-Light</code>, etc. <code>OPTForCausalLM</code> OPT <code>facebook/opt-1.3b</code>, <code>facebook/opt-66b</code>, etc. <code>OlmoForCausalLM</code> OLMo <code>allenai/OLMo-7B</code>, etc. <code>PhiForCausalLM</code> Phi <code>microsoft/phi-1_5</code>, <code>microsoft/phi-2</code>, etc. <code>Phi3ForCausalLM</code> Phi3 <code>microsoft/phi-3</code>, etc. <code>StableLmForCausalLM</code> StableLM <code>stabilityai/stablelm-3b-4e1t/</code>, etc. <code>StarCoder2ForCausalLM</code> StarCoder2 <code>bigcode/starcoder2-15b</code>, etc. <p>Note</p> <p>Models marked under the \u201cHybrid\u201d tab have been validated for use in the LPU-GPU Hybrid system. For Hybrid validation, the models have been tested with NVIDIA\u2019s Ampere, Hopper, and Ada Lovelace product lines, ensuring compatibility and performance across these architectures.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"troubleshooting/#1-timeout-error","title":"1. Timeout error","text":"<p>\uc6d0\uc778</p> <p>LPU\uac00 \uc751\ub2f5\ud558\uc9c0 \ubabb\ud55c \uc0c1\ud0dc\ub85c, \ub9ac\uc18c\uc2a4 \ucda9\ub3cc \ub4f1\uc73c\ub85c \uc778\ud574 Deadlock\uc774 \ubc1c\uc0dd\ud588\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4. </p> <p>\ud574\uacb0</p> <ul> <li><code>hyperdex-reset</code> \uc2e4\ud589  </li> <li>\uc9c0\uc18d\uc801\uc77c \uacbd\uc6b0 \uc11c\ubc84 \ub9ac\ubd80\ud2b8 \ubc0f <code>host-memory-access</code> \ud6c4 \uc7ac\uc2dc\ub3c4  </li> </ul> <p>\uc608\uc2dc \uc5d0\ub7ec \ub85c\uadf8: <pre><code>...\nProcessed prompts:   0%|                      | 0/1 [00:00&lt;?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s2025-07-31 13:19:18,798 - [ ERROR ] : Timeout occurred while waiting for the LPU to generate tokens.\n2025-07-31 13:19:18,798 - [ ERROR ] : Timeout occurred while waiting for the LPU to generate tokens.\n2025-07-31 13:19:18,803 - [ CRITICAL ] : Soft Reset Triggered! It takes about 3 seconds in multi LPU config...\n</code></pre></p>"},{"location":"troubleshooting/#2-multi-batch","title":"2. Multi-batch","text":"<p>\uc6d0\uc778</p> <p>\uc785\ub825 \ud504\ub86c\ud504\ud2b8\uac00 \ub2e4\uc911 \ubc30\uce58\ub85c \uc804\ub2ec\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</p> <p>\ud574\uacb0</p> <p>\uc785\ub825 \ubc30\uce58\ub97c 1\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.</p> <p>\uc608\uc2dc \uc5d0\ub7ec \ub85c\uadf8: <pre><code>...\n[rank0]:     input_tokens = torch.tensor(input_tokens,\n[rank0]: ValueError: expected sequence of length 6 at dim 1 (got 8)\nProcessed prompts:   0%|  \n</code></pre></p>"},{"location":"troubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"troubleshooting/#xrt","title":"XRT","text":"Components Status XOCL &amp; XCLMGMT Kernel Driver \u274c Failed XRT USERSPACE \u2705 Success MPD/MSD \u2705 Success"},{"location":"troubleshooting/#xocl-xclmgmt-kernel-driver-failed","title":"\uc624\ub958: XOCL &amp; XCLMGMT Kernel Driver Failed","text":"<p>\uc6d0\uc778</p> <p>Os <code>linux-image</code> \ubc84\uc804\uacfc <code>linux-headers</code> \uac04\uc758 \ubc84\uc804 \ubd88\uc77c\uce58  </p> <p>\ud574\uacb0</p> <p>\ub450 \ud328\ud0a4\uc9c0\ub97c \ub3d9\uc77c\ud55c \ubc84\uc804\uc73c\ub85c \ub9de\ucdb0\uc57c \ud569\ub2c8\ub2e4. </p> <pre><code>uname -r   # linux-image \ubc84\uc804 \ud655\uc778\napt list --installed | grep -i linux-headers  # linux-headers \ubc84\uc804 \ud655\uc778\n</code></pre>"},{"location":"troubleshooting/#xrt-error-kernel-arg-axi00_ptr0-is-not-set","title":"\uc624\ub958: [XRT] ERROR: Kernel arg <code>axi00_ptr0</code> is not set","text":"<p>\uc6d0\uc778</p> <p>\uc11c\ubc84 \uc7ac\ubd80\ud305 \ud6c4 host memory\uac00 \ud65c\uc131\ud654\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.</p> <p>\ud574\uacb0</p> <p>host memory enable \uc2a4\ud06c\ub9bd\ud2b8\ub97c sudo \uad8c\ud55c\uc774 \uc788\ub294 \uacc4\uc815\uc5d0\uc11c \uc2e4\ud589\ud569\ub2c8\ub2e4.</p> <pre><code>host-memory-access\n</code></pre>"},{"location":"troubleshooting/#xrt-error-unable-to-sync-bo-inputoutput-error","title":"\uc624\ub958: [XRT] ERROR: unable to sync BO: Input/output error","text":"<p>\uc6d0\uc778</p> <p>Ctrl+C \ub4f1 \uc758\ub3c4\uce58 \uc54a\uc740 \uc2dc\uadf8\ub110\uc774 \ub514\ubc14\uc774\uc2a4\ub85c \uc804\ub2ec\ub428  </p> <p>\ud574\uacb0</p> <p>hyperdex-toolchain \ud658\uacbd\uc5d0\uc11c reset\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.</p> <pre><code>hyperdex-reset\n</code></pre>"},{"location":"troubleshooting/#model-compile-complete","title":"\uc624\ub958: Model compile Complete! \uc774\ud6c4 \uc544\ubb34\ub7f0 \ucd9c\ub825 \uc5c6\uc74c","text":"<p>\uc6d0\uc778</p> <p>Deadlock \ubc1c\uc0dd  </p> <p>\ud574\uacb0</p> <p>hyperdex-toolchain \ud658\uacbd\uc5d0\uc11c reset\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.</p> <pre><code>hyperdex-reset\n</code></pre>"},{"location":"troubleshooting/#cuda","title":"CUDA","text":""},{"location":"troubleshooting/#failed-to-initialize-nvml-driverlibrary-version-mismatch","title":"\uc624\ub958: Failed to initialize NVML : Driver/library version mismatch","text":"<p>\uc6d0\uc778</p> <p>\ucee4\ub110\uc5d0 \ub85c\ub4dc\ub41c \ub4dc\ub77c\uc774\ubc84 \ubc84\uc804\uacfc <code>/usr/lib/</code> \ub0b4 NVML \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640\uc758 \ubc84\uc804\uc774 \ubd88\uc77c\uce58</p> <p>\ud574\uacb0</p> <p>NVIDIA \ubaa8\ub4c8 \uc5b8\ub85c\ub4dc \ud6c4 \uc7ac\ub85c\ub4dc (\uc0ac\uc6a9 \uc911\uc778 \ud504\ub85c\uc138\uc2a4\ub294 kill \ud6c4 \uc9c4\ud589)  </p> <pre><code>rmmod nvidia_uvm\nrmmod nvidia_drm\nrmmod nvidia_modeset\nrmmod nvidia\nmodprobe nvidia\n\nnvidia-smi\n</code></pre>"},{"location":"troubleshooting/#package","title":"Package","text":""},{"location":"troubleshooting/#attributeerror-memory_mapper-object-has-no-attribute-lib","title":"\uc624\ub958: AttributeError: 'memory_mapper' object has no attribute 'lib'","text":"<p>\uc6d0\uc778</p> <p>Torch \ubc84\uc804 mismatch (\ud544\uc218: 2.7.0)  </p> <p>\ud574\uacb0</p> <p>torch 2.7.0 \uc7ac\uc124\uce58  </p> <pre><code># LPU only env\nuv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cpu\n# LPU + GPU env\nuv pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126\n</code></pre>"},{"location":"troubleshooting/#runtimeerror-failed-to-make-network-connection-table","title":"\uc624\ub958: RuntimeError: Failed to make network connection table","text":"<p>\uc6d0\uc778</p> <p>\ub124\ud2b8\uc6cc\ud06c \ud14c\uc774\ube14 \uc0dd\uc131 \uc2e4\ud328  </p> <p>\ud574\uacb0</p> <ul> <li>network table \ud30c\uc77c \uc874\uc7ac \uc5ec\ubd80 \ud655\uc778  </li> <li>QSFP cabling \uc0c1\ud0dc \ud655\uc778 \ud6c4 \uc7ac\uc0dd\uc131  </li> </ul> <pre><code>ls -al /tmp/hyperdex/xclbin/table_bdf.json\n</code></pre> <p>\uc608\uc2dc \ucd9c\ub825: <pre><code>-rw-r--r-- 1 user user 233  9\uc6d4  3 15:56 /tmp/hyperdex/xclbin/table_bdf.json\n</code></pre></p> <p>\u2192 \ud30c\uc77c\uc774 \uc5c6\uc744 \uacbd\uc6b0 \uc7ac\uc0dd\uc131\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.  </p>"},{"location":"troubleshooting/#vllm","title":"vLLM","text":""},{"location":"troubleshooting/#curl-7-failed-to-connect-to-localhost-port-8001-after-0-ms-connection-refused","title":"\uc624\ub958: curl: (7) Failed to connect to localhost port 8001 after 0 ms: Connection refused","text":"<p>\uc6d0\uc778</p> <p>curl \uc694\uccad\uc774 \uc2e4\ud589\uc911\uc778 vLLM\uacfc \ub2e4\ub978 \ud3ec\ud2b8\ub85c \uc804\uc1a1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</p> <p>\ud574\uacb0</p> <p>\uc2e4\ud589 \uc911\uc778 vLLM \ud504\ub85c\uc138\uc2a4\uc758 \ud3ec\ud2b8\ub97c \ud655\uc778\ud569\ub2c8\ub2e4.  \uc2e4\ud589 \uc2dc \uc124\uc815\ud55c \ud3ec\ud2b8\uc640 \uac19\uc740 \uac12\uc73c\ub85c curl \uc694\uccad\uc744 \ubcf4\ub0c5\ub2c8\ub2e4.</p> <pre><code>ps -ef | grep -i vllm\n</code></pre>"},{"location":"useful_commands/","title":"Useful Commands","text":""},{"location":"useful_commands/#host-memory-access","title":"Host-Memory-Access","text":"<p>You need to allow LPUs to access to host memory when you boot the server. this command requires sudo priviliege.</p> <pre><code>$ host-memory-access\nHost-mem enabled successfully\nHost-mem enabled successfully\nHost-mem enabled successfully\nHost-mem enabled successfully\n</code></pre>"},{"location":"useful_commands/#system-management-interface","title":"System Management Interface","text":"<p>You can check the status of the device that is running. This code shows online devices and each memory uses.</p> <pre><code>$ watch -n 1 hyperdex-smi\nFri Sep  6 17:17:52 2024\n+-----------------------------------------------------------------------------+\n| HYPERDEX-SMI            XRT Version: 2022.2     HyperDex Version: 1.5.2     |\n+-----------------------------------------------------------------------------+\n| FPGA Name        Persistence-M| Bus-Id     Bitstream | Volatile Uncorr. ECC |\n|      Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | FPGA-Util Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|    0      XILINX U55C    Off  | 0000:b1:00.1      On |                  N/A |\n|       48C    P8    34W / 225W |      0MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    1      XILINX U55C    Off  | 0000:b2:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    2      XILINX U55C    Off  | 0000:ca:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|    3      XILINX U55C    Off  | 0000:cb:00.1     Off |                  N/A |\n|       34C    P8    16W / 225W |      0MiB /     0MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>This refreshes the display every second. To make the refresh rate faster, decrease 1 to a lower number(e.g. 0.5). The number on the most left side is the id of the LPU\u2122.</p>"},{"location":"useful_commands/#reset-the-lpu-device","title":"Reset the LPU device","text":"<p>Instead of pressing <code>Ctrl+C</code>, please reset the device. First, check the device id with the hyperdex-smi. Next reset the device with the following code.</p> <pre><code>$ hyperdex-reset\n</code></pre>"},{"location":"useful_commands/#create-network-table","title":"Create Network Table","text":"<p>This creates network table which is necessary to use multiple LPUs. <pre><code>$ hyperdex-net\n</code></pre></p>"}]}