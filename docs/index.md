<!---
Copyright 2023 The HyperAccel. All rights reserved.
-->

<p align="center">
    <br>
    <img src="./images/logo.png" width="400"/>
    <br>
</p>

# Welcome to HyperAccel Documentation

## About HyperAccel

**HyperAccel** is a pioneering semiconductor company specializing in the design and development of **LLM-optimized chips**. We focus on creating next-generation hardware solutions specifically engineered to accelerate large language model inference workloads.

Our mission is to revolutionize AI computing by delivering hardware that fundamentally understands the unique computational patterns and requirements of large language models, enabling unprecedented performance improvements in AI applications. HyperAccel addresses these specific requirements with purpose-built silicon architecture.

## LLM Processing Unit (LPU)

**LPU** is HyperAccel's breakthrough hardware architecture designed from the ground up for large language model inference. Unlike traditional accelerators that were originally designed for other computing tasks, the LPU is purpose-built to handle the specific computational patterns of LLMs.

### Key LPU Advantages

- **LLM-Specific Design**: Hardware optimized for transformer architectures and attention mechanisms
- **Latency-Optimized Architecture**: Specifically designed to minimize inference latency for LLM workloads
- **Energy Efficiency**: Delivers superior performance-per-watt compared to traditional accelerators like NVIDIA GPUs
- **High Scalability**: Efficiently handles models of varying sizes and complexities  


## HyperDex Software Stack

**HyperDex** is HyperAccel's comprehensive end-to-end (E2E) software solution that enables you to fully harness the power of LPU technology. It provides a complete ecosystem for running large language models efficiently on our specialized hardware.
The toolchain ensures a smooth transition from existing GPU-based workflows to LPU-optimized inference without requiring significant code changes.




## Getting Started

Ready to accelerate your LLM inference? This documentation covers:

- **[Installation Guide](install_hyperdex.md)**: Set up HyperDex on your system
- **[Quick Start](quick_start.md)**: Run your first model in minutes

## Support

Need help or have questions? Our team is here to support you every step of the way. Please [contact us](mailto:contact@hyperaccel.ai) for assistance.

